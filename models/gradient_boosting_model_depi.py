# -*- coding: utf-8 -*-
"""Gradient boosting model DEPI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12oUfcvluE4F2CIQQkGQRzT_FfD2Gi_p1
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_recall_fscore_support
from sklearn.feature_selection import SelectFromModel
import warnings
from google.colab import files
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/Ultimate_Final.csv')

print("Dataset shape:", df.shape)
print("\nMissing values before imputation:")
print(df.isnull().sum())

# More sophisticated missing value handling
df_filled = df.copy()
numeric_cols = df_filled.select_dtypes(include=[np.number]).columns

# Use median for robustness to outliers
df_filled[numeric_cols] = df_filled[numeric_cols].fillna(df_filled[numeric_cols].median())

print(f"\nMissing values after imputation: {df_filled.isnull().sum().sum()}")

# Define target - Accident Severity with better threshold
severity_threshold = df_filled['Severity'].quantile(0.7)  # More balanced approach
df_filled['High_Severity'] = (df_filled['Severity'] > severity_threshold).astype(int)

print(f"\nTarget distribution:")
print(df_filled['High_Severity'].value_counts())
print(f"Severity threshold (70th percentile): {severity_threshold:.2f}")

"""#**Advanced feature engineering**"""

# Create more sophisticated features
df_enhanced = df_filled.copy()

# Time-based features (enhanced)
df_enhanced['Is_Rush_Hour'] = ((df_enhanced['Month'].between(6, 9)) &
                              (df_enhanced['Traffic_Event_Count'] > df_enhanced['Traffic_Event_Count'].median())).astype(int)

# Weather severity index (enhanced)
df_enhanced['Weather_Severity_Index'] = (
    (10 - df_enhanced['Visibility(mi)'].clip(0, 10)) * 0.3 +
    np.log1p(df_enhanced['Wind_Speed(mph)']) * 0.3 +
    np.log1p(df_enhanced['Precipitation(in)'] * 50) * 0.2 +
    (100 - df_enhanced['Humidity(%)']).clip(0, 100) * 0.2
)

# Traffic congestion score (enhanced)
df_enhanced['Traffic_Congestion_Score'] = (
    np.log1p(df_enhanced['DelayFromTypicalTraffic(mins)']) * 0.5 +
    np.log1p(df_enhanced['DelayFromFreeFlowSpeed(mins)']) * 0.3 +
    np.log1p(df_enhanced['Traffic_Event_Count']) * 0.2
)

# Location features (enhanced)
df_enhanced['Lat_Cluster'] = (df_enhanced['Start_Lat'] * 20).round() / 20
df_enhanced['Lng_Cluster'] = (df_enhanced['Start_Lng'] * 20).round() / 20

# Accident density (enhanced)
accident_density = df_enhanced.groupby(['Lat_Cluster', 'Lng_Cluster']).size()
df_enhanced['Accident_Density'] = df_enhanced.apply(
    lambda row: accident_density.get((row['Lat_Cluster'], row['Lng_Cluster']), 0), axis=1
)
df_enhanced['Log_Accident_Density'] = np.log1p(df_enhanced['Accident_Density'])

# Interaction features
df_enhanced['Weather_Traffic_Interaction'] = df_enhanced['Weather_Severity_Index'] * df_enhanced['Traffic_Congestion_Score']
df_enhanced['Visibility_Humidity_Interaction'] = df_enhanced['Visibility(mi)'] * df_enhanced['Humidity(%)']

# Polynomial features
df_enhanced['Temperature_Squared'] = df_enhanced['Temperature(F)'] ** 2
df_enhanced['Wind_Speed_Squared'] = df_enhanced['Wind_Speed(mph)'] ** 2

# Time-related features
df_enhanced['Season'] = df_enhanced['Month'] % 12 // 3
df_enhanced['Is_Weekend'] = (df_enhanced['Month'] % 7 >= 5).astype(int)  # Mock weekend

print(f"Original features: {len(df_filled.columns)}")
print(f"Enhanced features: {len(df_enhanced.columns)}")

"""#**Feature selection**"""

features = [
    # Original features
    'Month', 'Accident_Count', 'Start_Lat', 'Start_Lng', 'Distance(mi)',
    'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',
    'Wind_Speed(mph)', 'Traffic_Event_Count', 'Traffic_Severity',
    'DelayFromTypicalTraffic(mins)', 'DelayFromFreeFlowSpeed(mins)',

    # Engineered features
    'Is_Rush_Hour', 'Weather_Severity_Index', 'Traffic_Congestion_Score',
    'Accident_Density', 'Log_Accident_Density', 'Weather_Traffic_Interaction',
    'Visibility_Humidity_Interaction', 'Temperature_Squared', 'Wind_Speed_Squared',
    'Season', 'Is_Weekend'
]

X = df_enhanced[features]
y = df_enhanced['High_Severity']

print(f"\nUsing {len(features)} features:")
new_features = [f for f in features if f not in [
    'Month', 'Accident_Count', 'Start_Lat', 'Start_Lng', 'Distance(mi)',
    'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',
    'Wind_Speed(mph)', 'Traffic_Event_Count', 'Traffic_Severity',
    'DelayFromTypicalTraffic(mins)', 'DelayFromFreeFlowSpeed(mins)'
]]

for i, feature in enumerate(features, 1):
    marker = "" if feature in new_features else "  "
    print(f"  {i:2d}. {marker} {feature}")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nData split:")
print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

"""# **Advanced preprocessing**"""

# Power transform for skewed features
power_transformer = PowerTransformer(method='yeo-johnson')
skewed_features = ['Accident_Count', 'Distance(mi)', 'Traffic_Event_Count',
                   'DelayFromTypicalTraffic(mins)', 'Accident_Density']

X_train_skewed = power_transformer.fit_transform(X_train[skewed_features])
X_test_skewed = power_transformer.transform(X_test[skewed_features])

X_train_transformed = X_train.copy()
X_test_transformed = X_test.copy()

for i, col in enumerate(skewed_features):
    X_train_transformed[col] = X_train_skewed[:, i]
    X_test_transformed[col] = X_test_skewed[:, i]

# Scale all features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_transformed)
X_test_scaled = scaler.transform(X_test_transformed)


def evaluate_model(name, model, is_scaled, X, y_true):
    """Comprehensive model evaluation with multiple metrics"""

    if is_scaled:
        X_eval = X
        y_pred = model.predict(X_eval)
        y_pred_proba = model.predict_proba(X_eval)[:, 1]
    else:
        X_eval = X
        y_pred = model.predict(X_eval)
        y_pred_proba = model.predict_proba(X_eval)[:, 1]

    accuracy = accuracy_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
    cm = confusion_matrix(y_true, y_pred)

    print(f"\n{'='*50}")
    print(f" {name.upper()}")
    print(f"{'='*50}")
    print(f" Accuracy:    {accuracy:.4f} ({accuracy*100:.1f}%)")
    print(f" ROC-AUC:     {roc_auc:.4f}")
    print(f" Precision:   {precision:.4f}")
    print(f" Recall:      {recall:.4f}")
    print(f"  F1-Score:    {f1:.4f}")
    print(f" Test Size:   {len(y_true)}")

    print(f"\nClassification Report:")
    print(classification_report(y_true, y_pred, digits=3))

    print(f" Confusion Matrix:")
    print(f"               Predicted")
    print(f"              Low     High")
    print(f"Actual Low  [{cm[0,0]:>5}   {cm[0,1]:>5}]")
    print(f"Actual High [{cm[1,0]:>5}   {cm[1,1]:>5}]")

    correct = cm[0,0] + cm[1,1]
    total = cm.sum()
    print(f"\n Performance:")
    print(f"  Correct: {correct:,} ({correct/total*100:.1f}%)")
    print(f"  False Alarms: {cm[0,1]:,}")
    print(f"  Missed Dangers: {cm[1,0]:,}")

    return {
        'accuracy': accuracy,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

"""#**ENHANCED GRADIENT BOOSTING WITH FEATURE SELECTION**"""

# More robust feature selection
feature_selector = RandomForestClassifier(n_estimators=200, random_state=42)
feature_selector.fit(X_train_transformed, y_train)

# Use median threshold instead of mean
selector = SelectFromModel(feature_selector, threshold='median', prefit=True)
X_train_selected = selector.transform(X_train_transformed)
X_test_selected = selector.transform(X_test_transformed)

print(f" Selected {X_train_selected.shape[1]} features from {X_train_transformed.shape[1]}")

# More optimized parameters for selected features
gb_model = GradientBoostingClassifier(
    n_estimators=600,
    learning_rate=0.08,
    max_depth=7,
    min_samples_split=8,
    min_samples_leaf=3,
    subsample=0.85,
    max_features=0.7,
    random_state=42
)

gb_model.fit(X_train_selected, y_train)

# Cross-validation
gb_cv_scores = cross_val_score(gb_model, X_train_selected, y_train, cv=5, scoring='accuracy')
print(f" Enhanced Gradient Boosting trained! CV Accuracy: {gb_cv_scores.mean():.4f} (+/- {gb_cv_scores.std() * 2:.4f})")

# Evaluation
gb_results = evaluate_model("Enhanced GB with Feature Selection", gb_model, False, X_test_selected, y_test)

import joblib
import pickle

print("\n" + "="*60)
print("SAVING TRAINED MODELS")
print("="*60)

# Save the main model
joblib.dump(gb_model, 'severity_gb_model.pkl')
print(" Gradient Boosting model saved: severity_gb_model.pkl")

# Save the feature selector
joblib.dump(selector, 'feature_selector.pkl')
print(" Feature selector saved: feature_selector.pkl")

# Save the scaler
joblib.dump(scaler, 'scaler.pkl')
print(" Scaler saved: scaler.pkl")

# Save the power transformer
joblib.dump(power_transformer, 'power_transformer.pkl')
print(" Power transformer saved: power_transformer.pkl")

# Save the feature names (important!)
with open('feature_names.pkl', 'wb') as f:
    pickle.dump(features, f)
print(" Feature names saved: feature_names.pkl")

# Save the preprocessing objects
preprocessing_objects = {
    'severity_threshold': severity_threshold,
    'skewed_features': skewed_features
}
joblib.dump(preprocessing_objects, 'preprocessing_objects.pkl')
print(" Preprocessing objects saved: preprocessing_objects.pkl")

# Download all files
from google.colab import files

files_to_download = [
    'severity_gb_model.pkl',
    'feature_selector.pkl',
    'scaler.pkl',
    'power_transformer.pkl',
    'feature_names.pkl',
    'preprocessing_objects.pkl'
]

for file in files_to_download:
    files.download(file)
    print(f" Downloaded: {file}")

print("\nALL MODELS SAVED AND DOWNLOADED!")
print("You can now use these in your Streamlit app!")